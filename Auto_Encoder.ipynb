{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto-Encoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wiemChakroun/ENIT_DeepGenerativeModels/blob/master/Auto_Encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ClFN4uAV8P5",
        "colab_type": "text"
      },
      "source": [
        "**Autoencoder** is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
        "Autoencoder, by design, reduces data dimensions by learning how to **ignore the noise in the data**. \n",
        "[Paper link](http://proceedings.mlr.press/v27/baldi12a/baldi12a.pdf)\n",
        "\n",
        "\n",
        "# Use cases:\n",
        "\n",
        "1.   Autoencoder for Anomaly Detection.\n",
        "2.   Autoencoder for image Denoising.\n",
        "![alt text](https://static.packt-cdn.com/products/9781788399906/graphics/f9b44226-662e-43a1-aaa8-f9f952d8ce60.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmMf31TxYMdl",
        "colab_type": "text"
      },
      "source": [
        "Autoencoder Components:\n",
        "1.    **Encoder**: In which the model learns how to reduce the input dimensions and compress the input data into an encoded representation.\n",
        "2.   **Bottleneck**: which is the layer that contains the compressed representation of the input data. This is the lowest possible dimensions of the input data.\n",
        "3.   **Decoder**: In which the model learns how to reconstruct the data from the encoded representation to be as close to the original input as possible.\n",
        "4. **Reconstruction Loss**: This is the method that measures measure how well the decoder is performing and how close the output is to the original input.\n",
        "\n",
        "The training then involves using back propagation in order to minimize the network’s reconstruction loss.\n",
        "![alt text](https://miro.medium.com/max/1838/0*yGqTBMopqHbR0fcF.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s86xnmSpVMBk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spVXt420VB0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMITakE-kZ2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "learning_rate = 1e-4\n",
        "epochs = 16"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpaTSQ33kE57",
        "colab_type": "text"
      },
      "source": [
        "## Load and prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5ji_fXjj01V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(training_features, _), (test_features, _) = tf.keras.datasets.mnist.load_data()\n",
        "training_features = training_features / np.max(training_features)\n",
        "training_features = training_features.reshape(training_features.shape[0],\n",
        "                                              training_features.shape[1] * training_features.shape[2])\n",
        "training_features = training_features.astype('float32')\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices(training_features)\n",
        "training_dataset = training_dataset.batch(batch_size)\n",
        "training_dataset = training_dataset.shuffle(training_features.shape[0])\n",
        "training_dataset = training_dataset.prefetch(batch_size * 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95CaAg5th-XH",
        "colab_type": "text"
      },
      "source": [
        "## Data flow and model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ez7Vfca2eisp",
        "colab_type": "text"
      },
      "source": [
        "The encoding is done by passing data input x to the encoder’s hidden layer h in order to learn the data representation z = f(h(x)) (z compressed data). We can implement the `Encoder` layer as follows."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1mQtd4bV5Zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, intermediate_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.hidden_layer = tf.keras.layers.Dense(\n",
        "      units=intermediate_dim,\n",
        "      activation=tf.nn.relu,\n",
        "      kernel_initializer='he_uniform'\n",
        "    )\n",
        "    self.output_layer = tf.keras.layers.Dense(\n",
        "      units=intermediate_dim,\n",
        "      activation=tf.nn.sigmoid\n",
        "    )\n",
        "    \n",
        "  def call(self, input_features):\n",
        "    activation = self.hidden_layer(input_features)\n",
        "    return self.output_layer(activation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIVZSlP9fXJ3",
        "colab_type": "text"
      },
      "source": [
        "The decoding is done by passing the lower dimension representation z to the decoder’s hidden layer h in order to reconstruct the data x to its original dimension x = f(h(z)). We can implement the `decoder` layer as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0kpjIfncXzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, intermediate_dim, original_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.hidden_layer = tf.keras.layers.Dense(\n",
        "      units=intermediate_dim,\n",
        "      activation=tf.nn.relu,\n",
        "      kernel_initializer='he_uniform'\n",
        "    )\n",
        "    self.output_layer = tf.keras.layers.Dense(\n",
        "      units=original_dim,\n",
        "      activation=tf.nn.sigmoid\n",
        "    )\n",
        "  \n",
        "  def call(self, code):\n",
        "    activation = self.hidden_layer(code)\n",
        "    return self.output_layer(activation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_SDxOUlgt8J",
        "colab_type": "text"
      },
      "source": [
        "Building the Autoencoder model\n",
        "We can now build the autoencoder model by instantiating the `Encoder` and the Decoder layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvtAcOQvgOcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Autoencoder(tf.keras.Model):\n",
        "  def __init__(self, intermediate_dim, original_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.encoder = Encoder(intermediate_dim=intermediate_dim)\n",
        "    self.decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
        "  \n",
        "  def call(self, input_features):\n",
        "    code = self.encoder(input_features)\n",
        "    reconstructed = self.decoder(code)\n",
        "    return reconstructed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L--2ZqbriNR1",
        "colab_type": "text"
      },
      "source": [
        "## Model training\n",
        "The used loss during training is mean-squared error function. we compare the reconstructed data x-hat and the original data x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dk4egMM8hx-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss(model, original):\n",
        "  reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original)))\n",
        "  return reconstruction_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDE1YceLiuqi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(loss, model, opt, original):\n",
        "  with tf.GradientTape() as tape:\n",
        "    gradients = tape.gradient(loss(model, original), model.trainable_variables)\n",
        "    gradient_variables = zip(gradients, model.trainable_variables)\n",
        "    opt.apply_gradients(gradient_variables)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNCEZDjPi_Q1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder = Autoencoder(intermediate_dim=64, original_dim=784)\n",
        "opt = tf.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "writer = tf.summary.create_file_writer('tmp')\n",
        "\n",
        "with writer.as_default():\n",
        "  with tf.summary.record_if(True):\n",
        "    for epoch in range(epochs):\n",
        "      for step, batch_features in enumerate(training_dataset):\n",
        "        train(loss, autoencoder, opt, batch_features)\n",
        "        loss_values = loss(autoencoder, batch_features)\n",
        "        original = tf.reshape(batch_features, (batch_features.shape[0], 28, 28, 1))\n",
        "        reconstructed = tf.reshape(autoencoder(tf.constant(batch_features)), (batch_features.shape[0], 28, 28, 1))\n",
        "        tf.summary.scalar('loss', loss_values, step=step)\n",
        "        tf.summary.image('original', original, max_outputs=10, step=step)\n",
        "        tf.summary.image('reconstructed', reconstructed, max_outputs=10, step=step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AscVVw95oNuU",
        "colab_type": "text"
      },
      "source": [
        "References: \n",
        "https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7"
      ]
    }
  ]
}